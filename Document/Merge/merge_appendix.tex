\newpage
\appendix
\section*{Appendix}
\section{Latent Matrix Factorization}
 We talked about Latent Matrix Factorization in the document earlier. In this section, we will go through the basics of matrix factorization~\cite{model_ref3}. Assume that we are dealing with user ratings (score in the range of 1 to 5) of items in a recommendation system.

\paragraph{Basic Idea}
 In recommendation system like Netflix or MovieLens, there are a group of users and a set of items. Given that each user has rated only some items in the system, we would like to predict how the users would rate the other not yet rated items, so that we can make appropriate recommendations to the users. We can represent all the information we have about the existing ratings in a matrix. Assume that we have 5 users and 4 items, and ratings range from 1 to 5 (integer values), the matrix may look something like this:

\begin{table}[ht]
\caption{User Ratings}
\centering % used for centering table
\begin{tabular}{c c c c c}
\hline\hline %inserts double horizontal lines
& D1 & D2 & D3 & D4 \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
U1 & 5 & 3 & - & 1\\ % inserting body of the table
U2 & 4 & - & - & 1\\
U3 & 1 & 1 & - & 5\\
U4 & 1 & - & - & 4\\
U5 & - & 1 & - & 4\\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

 The task of predicting the missing ratings can be considered as filling in the blanks such that the values would be consistent with the existing ratings in the matrix.
 
 The basic idea behind using matrix factorization is that there should be some latent features that can predict how a user rates an item. For example, two users would give high ratings to a certain movie if they both like the actors/actresses of the movie, or if the movie is an comedy movie, which is a genre preferred by both users. Hence, if we can discover these latent features, we should be able to determine a rating with respect to a certain user and a certain item, because the features associated with the user should match with the features associated with the item.

\paragraph{The Mathematics of Matrix Factorization}

 Having discussed the intuition behind matrix factorization, we can now go on to work on the mathematics. Firstly, we have a set U of users, and a set D of items. Let $\mathbf{R}$ of size $|U| \times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover $K$ latent features. Our task, then, is to find two matrices $\mathbf{P}$ (a $|U| \times K$ matrix) and $\mathbf{Q}$ (a $|D| \times K$ matrix) such that their product approximates $\mathbf{R}$:
  \begin{gather*}
    R \approx P \times Q^{T} = \widehat{R}
  \end{gather*}

 In this way, each row of $\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\mathbf{Q}$ would represent the strength of the associations between an item and the features. To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of the two vectors corresponding to $u_i$ and $d_j$:
  \begin{gather*}
    r_{ij} = p_i^{T}q_j = \sum_{k=1}^{K}p_{ik}q_{kj}
  \end{gather*}

 Now, we have to find a way to obtain $\mathbf{P}$ and $\mathbf{Q}$. One way to approach this problem is to first initialize the two matrices with some values, calculate how 'different' their product is from $\mathbf{M}$, and then try to minimize this difference iteratively. Such a method is called gradient descent, aiming at finding a local minimum of the difference.

 Here we consider the squared error because the estimated rating can be either higher or lower than the real rating.

 To minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. In other words, we need to know the gradient at the current values, and therefore we differentiate the above equation with respect to these two variables separately:
	\begin{gather*}
      \frac{\partial}{\partial p_{ik}}e_{ij}^2 = -2(r_{ij} - \widehat{r_{ij}})(q_{kj}) = -2e_{ij}q_{kj} \\
      \frac{\partial}{\partial q_{ik}}e_{ij}^2 = -2(r_{ij} - \widehat{r_{ij}})(p_{ik}) = -2e_{ij}p_{ik}
  \end{gather*}

 Having obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:
\begin{gather*}
      p^`_{ik} = p_{ik} + \alpha\frac{\partial}{\partial p_{ik}}e_{ij}^2 = p_{ik} + 2\alpha e_{ij}q_{kj} \\
      q^`_{kj} = q_{kj} + \alpha\frac{\partial}{\partial q_{kj}}e_{ij}^2 = q_{kj} + 2\alpha e_{ij}p_{ik}
  \end{gather*}

 Here, $\alpha$ is a constant whose value determines the rate of approaching the minimum. Usually we will choose a small value for $\alpha$, say 0.0002. This is because if we make too large a step towards the minimum we may run into the risk of missing the minimum and end up oscillating around the minimum.

 A question might have come to your mind by now: if we find two matrices $\mathbf{P}$ and $\mathbf{Q}$ such that $\mathbf{P}$ $\times$ $\mathbf{Q}$ approximates $\mathbf{R}$, isnâ€™t that our predictions of all the unseen ratings will all be zeros? In fact, we are not really trying to come up with $\mathbf{P}$ and $\mathbf{Q}$ such that we can reproduce $\mathbf{R}$ exactly. Instead, we will only try to minimize the errors of the observed user-item pairs. In other words, if we let T be a set of tuples, each of which is in the form of $(u_i, d_j, r_{ij})$, such that T contains all the observed user-item pairs together with the associated ratings, we are only trying to minimize every $e_{ij}$ for $(u_i, d_j, r_{ij}) \in T$. (In other words, T is our set of training data.) As for the rest of the unknowns, we will be able to determine their values once the associations between the users, items and features have been learnt.

 Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.
  \begin{gather*}
    E = p_i^{T}q_j = \sum_{(u_i,d_j,r_{ij})\in T} e_{ij} = \sum_{(u_i,d_j,r_{ij})\in T} (r_{ij} - \sum_{k=1}^{K}p_{ik}q_{kj})^2
  \end{gather*}
\paragraph{Regularization}

 The above algorithm is a very basic algorithm for factorizing a matrix. There are a lot of methods to make things look more complicated. A common extension to this basic algorithm is to introduce regularization to avoid over-fitting. This is done by adding a parameter $\beta$ and modify the squared error as follows:
  \begin{gather*}
    e_{ij}^2 = (r_{ij} - \sum_{k=1}^{K}p_{ik}q_{kj})^2 + \frac{\beta}{2}\sum_{k=1}^{K}(||P||^2 + ||Q||^2)^2
  \end{gather*}

 In other words, the new parameter $\beta$ is used to control the magnitudes of the user-feature and item-feature vectors such that P and Q would give a good approximation of R without having to contain large numbers. In practice, $\beta$ is set to some values in the range of 0.02. The new update rules for this squared error can be obtained by a procedure similar to the one described above. The new update rules are as follows.
\begin{gather*}
      p^`_{ik} = p_{ik} + \alpha\frac{\partial}{\partial p_{ik}}e_{ij}^2 = p_{ik} + \alpha(2e_{ij}q_{kj} - \beta p_{ik}) \\
      q^`_{kj} = q_{kj} + \alpha\frac{\partial}{\partial q_{kj}}e_{ij}^2 = q_{kj} + \alpha(2e_{ij}p_{ik} - \beta q_{kj})
  \end{gather*}
\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Study the code in \textit{LatentMatrixFactorization.m} which implements the Latent Matrix Factorization technique. Use \textit{LMFCall.m} to call this method. Do you get interesting and/or useful recommendations for the new user based on her ratings for a small set of movies? Try and add/change the ratings for the new user and see how the recommendations change. Try different values of regularization parameters in \textit{LatentMatrixFactorization.m} and observe the differences in new user ratings.
\end{myremark}
\section{Code and Function Walkthrough}
Once you unzip the files in a directory $RecommenderSystemLab$, it will contain the following four folders. Add the directory $RecommenderSystemLab$ and its subfolders to your current matlab path.
\begin{enumerate}
\item \textbf{Useful Scripts} - This folder contains basic read write function implementations.
\begin{itemize}
\item \textbf{ConvertUDataToMatrix.m} - Contains the function to read a database and convert it into a user*movies matrix with uninitialized value set to zero.
\item \textbf{GetMovieNameDatabase.m} - Contains the function call to read the movie names for each movie Id.
\end{itemize}
\item \textbf{LabFunctions} - This folder contains basic functions used throughout the lab.
\begin{itemize}
\item \textbf{CosSimVecMatrix.m} - This function generates the Cosine similarity between a user and other users based on their ratings.
\item \textbf{PCSimVecMatrix.m} - This function generates the Pearson Correlation similarity between a user and other users based on their ratings.
\item \textbf{AverageValueBasedMatrixCompletion.m} - This function is the implementation of Average Value Based Matrix Completion method.
\item \textbf{Top5Accuracy.m} - This function takes in the test vector and predicted vector to calculate how many top five movies in the test vector were predicted by the predicted vector.
\item \textbf{StepErrorFunction.m} - This function calculates the number of good and bad movies in a test vector that were not predicted as good and bad in the predicted vector.
\item \textbf{IncrementalLowRankCompletion.m} - This function implements the incremental low rank completion method.
\item \textbf{PrintOutMoivesOfAUser.m} - Given a 1682 length vector and a movie database, this function prints out the names of the movie rated by the user. The input representation can be changed using a mode flag.
\item \textbf{TopKEigenValues.m} - Given a threshold with respect to the maximum sigma values and a diagonal matrix of sigma values, this matrix returns a matrix with sigma values below the threshold set to zero.
\item \textbf{FillZeroEntryWithAverageInARow.m} - Function used by Average Value Based Matrix Completion for Matrix Initialization.
\item \textbf{LatentMatrixFactorization.m} - This function implements the technique for Matrix Factorization.
\item \textbf{LMFCall.m} - This calls the LatentMatrixFactorization function to factorize the given matrix.
\end
{itemize}
\item \textbf{ModelBasedPrediction} - This folder has the final exercise of the Model Based System
\begin{itemize}
\item \textbf{EvaluateModelBasedSVD.m} - This function runs the Model Based SVD for the entire movie lens dataset, parameters are configurable.
\item \textbf{ModelBasedPredictionTest.m} - EvaluateModelBasedSVD calls this function for each customer to generate the prediction.
\end{itemize}
\item \textbf{Examples} - This folder has all the toy examples used in the Lab.
\begin{itemize}
\item \textbf{UserBasedPredictionToy.m} - contains code for generating prediction in Exercise 5.
\item \textbf{UserBasedPredictionReal\_kn.m and UserBasedPredictionReal\_kn.m} - are the implementations of user-based recommendation for reference.
\item \textbf{ToyExampleAverageValueSVD.m} - Contains code to implement Average Value SVD.
\item \textbf{ToyExampleIterativeSVD.m} - Contains code to implement Iterative Value SVD.
\item \textbf{ToyExampleModelSVDMotivation.m} - Contains example code to describe issues with Sparsity and a potential solution.
\item \textbf{ToyExampleModelSVDMotivation2.m} - Contains example code to describe issues related to Synonym and a potential solution.
 
\end{itemize}
\end{enumerate}
