\newpage
\appendix
\section*{Appendix}
\section{Latent Matrix Factorization}
There are numerous algorithms for generating recommendations. While user-based or item-based collaborative filtering methods are easy and intuitive, matrix factorization methods are usually more efficient because they allow us to determine the latent features underlying the interactions between users and items. Matrix factorization is just a mathematical tool for playing around with matrices, and is thus relevant in many situations where one would like to discover something hidden in the data.

Here, we will go through the basics of matrix factorization, and then we will look into a simple implementation in Python. Assume that we are dealing with user ratings (score in the range of 1 to 5) of items in a recommendation system.

\paragraph{Basic Idea}
Matrix factorization is basically factorizing a matrix, i.e. finding out two (or more) matrices such that their multiplication will result in the original matrix.

Matrix factorization can be used to identify latent features underlying the interactions between two different kinds of entities. One of it’s obvious application is to predict ratings (or score) in collaborative filtering.

In recommendation system like Netflix or MovieLens, there are a group of users and a set of items. Given that each users have rated only some items in the system, we would like to predict how the users would rate the other not yet rated items, so that we can make appropriate recommendations to the users. We can represent all the information we have about the existing ratings in a matrix. Assume that we have 5 users and 10 items, and ratings range from 1 to 5 (integer values), the matrix may look something like this:

\begin{table}[ht]
\caption{User Ratings}
\centering % used for centering table
\begin{tabular}{c c c c c}
\hline\hline %inserts double horizontal lines
& D1 & D2 & D3 & D4 \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
U1 & 5 & 3 & - & 1\\ % inserting body of the table
U2 & 4 & - & - & 1\\
U3 & 1 & 1 & - & 5\\
U4 & 1 & - & - & 4\\
U5 & - & 1 & - & 4\\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

The task of predicting the missing ratings can be considered as filling in the blanks such that the values would be consistent with the existing ratings in the matrix.
 
The basic idea behind using matrix factorization is that there should be some latent features that can predict how a user rates an item. For example, two users would give high ratings to a certain movie if they both like the actors/actresses of the movie, or if the movie is an comedy movie, which is a genre preferred by both users. Hence, if we can discover these latent features, we should be able to determine a rating with respect to a certain user and a certain item, because the features associated with the user should match with the features associated with the item.

\paragraph{The Mathematics of Matrix Factorization}

Having discussed the intuition behind matrix factorization, we can now go on to work on the mathematics. Firstly, we have a set U of users, and a set D of items. Let $\mathbf{R}$ of size $|U| \times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover $K$ latent features. Our task, then, is to find two matrices $\mathbf{P}$ (a $|U| \times K$ matrix) and $\mathbf{Q}$ (a $|D| \times K$ matrix) such that their product approximates $\mathbf{R}$:
  \begin{gather*}
    R \approx P \times Q^{T} = \widehat{R}
  \end{gather*}

 In this way, each row of $\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\mathbf{Q}$ would represent the strength of the associations between an item and the features. To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of the two vectors corresponding to $u_i$ and $d_j$:
  \begin{gather*}
    r_{ij} = p_i^{T}q_j = \sum_{k=1}^{K}p_{ik}q_{kj}
  \end{gather*}

 Now, we have to find a way to obtain $\mathbf{P}$ and $\mathbf{Q}$. One way to approach this problem is the first initialize the two matrices with some values, calculate how `different’ their product is to $\mathbf{M}$, and then try to minimize this difference iteratively. Such a method is called gradient descent, aiming at finding a local minimum of the difference.

 Here we consider the squared error because the estimated rating can be either higher or lower than the real rating.

 To minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. In other words, we need to know the gradient at the current values, and therefore we differentiate the above equation with respect to these two variables separately:
	\begin{gather*}
      \frac{\partial}{\partial p_{ik}}e_{ij}^2 = -2(r_{ij} - \widehat{r_{ij}})(q_{kj}) = -2e_{ij}q_{kj} \\
      \frac{\partial}{\partial q_{ik}}e_{ij}^2 = -2(r_{ij} - \widehat{r_{ij}})(p_{ik}) = -2e_{ij}p_{ik}
  \end{gather*}

 Having obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:
\begin{gather*}
      p^`_{ik} = p_{ik} + \alpha\frac{\partial}{\partial p_{ik}}e_{ij}^2 = p_{ik} + 2\alpha e_{ij}q_{kj} \\
      q^`_{kj} = q_{kj} + \alpha\frac{\partial}{\partial q_{kj}}e_{ij}^2 = q_{kj} + 2\alpha e_{ij}p_{ik}
  \end{gather*}

 Here, $\alpha$ is a constant whose value determines the rate of approaching the minimum. Usually we will choose a small value for $\alpha$, say 0.0002. This is because if we make too large a step towards the minimum we may run into the risk of missing the minimum and end up oscillating around the minimum.

 A question might have come to your mind by now: if we find two matrices $\mathbf{P}$ and $\mathbf{Q}$ such that $\mathbf{P}$ $\times$ $\mathbf{Q}$ approximates $\mathbf{R}$, isn’t that our predictions of all the unseen ratings will all be zeros? In fact, we are not really trying to come up with $\mathbf{P}$ and $\mathbf{Q}$ such that we can reproduce $\mathbf{R}$ exactly. Instead, we will only try to minimize the errors of the observed user-item pairs. In other words, if we let T be a set of tuples, each of which is in the form of $(u_i, d_j, r_{ij})$, such that T contains all the observed user-item pairs together with the associated ratings, we are only trying to minimize every $e_{ij}$ for $(u_i, d_j, r_{ij}) \in T$. (In other words, T is our set of training data.) As for the rest of the unknowns, we will be able to determine their values once the associations between the users, items and features have been learnt.

 Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.
  \begin{gather*}
    E = p_i^{T}q_j = \sum_{(u_i,d_j,r_{ij})\in T} e_{ij} = \sum_{(u_i,d_j,r_{ij})\in T} (r_{ij} - \sum_{k=1}^{K}p_{ik}q_{kj})^2
  \end{gather*}
\paragraph{Regularization}

 The above algorithm is a very basic algorithm for factorizing a matrix. There are a lot of methods to make things look more complicated. A common extension to this basic algorithm is to introduce regularization to avoid over-fitting. This is done by adding a parameter $\beta$ and modify the squared error as follows:
  \begin{gather*}
    e_{ij}^2 = (r_{ij} - \sum_{k=1}^{K}p_{ik}q_{kj})^2 + \frac{\beta}{2}\sum_{k=1}^{K}(||P||^2 + ||Q||^2)^2
  \end{gather*}

 In other words, the new parameter $\beta$ is used to control the magnitudes of the user-feature and item-feature vectors such that P and Q would give a good approximation of R without having to contain large numbers. In practice, $\beta$ is set to some values in the range of 0.02. The new update rules for this squared error can be obtained by a procedure similar to the one described above. The new update rules are as follows.
\begin{gather*}
      p^`_{ik} = p_{ik} + \alpha\frac{\partial}{\partial p_{ik}}e_{ij}^2 = p_{ik} + \alpha(2e_{ij}q_{kj} - \beta p_{ik}) \\
      q^`_{kj} = q_{kj} + \alpha\frac{\partial}{\partial q_{kj}}e_{ij}^2 = q_{kj} + \alpha(2e_{ij}p_{ik} - \beta q_{kj})
  \end{gather*}
\paragraph{Implementation in Python}
\begin{lstlisting}
import numpy

def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):
    Q = Q.T
    for step in xrange(steps):
        for i in xrange(len(R)):
            for j in xrange(len(R[i])):
                if R[i][j] > 0:
                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])
                    for k in xrange(K):
                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])
                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])
        eR = numpy.dot(P,Q)
        e = 0
        for i in xrange(len(R)):
            for j in xrange(len(R[i])):
                if R[i][j] > 0:
                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)
                    for k in xrange(K):
                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))
        if e < 0.001:
            break
    return P, Q.T
\end{lstlisting}

 We can try to apply it to our example mentioned above and see what we would get. Below is a code snippet in Python for running the example.
\begin{lstlisting}
R = [
     [5,3,0,1],
     [4,0,0,1],
     [1,1,0,5],
     [1,0,0,4],
     [0,1,5,4],
    ]

R = numpy.array(R)

N = len(R)
M = len(R[0])
K = 2

P = numpy.random.rand(N,K)
Q = numpy.random.rand(M,K)

nP, nQ = matrix_factorization(R, P, Q, K)
nR = numpy.dot(nP, nQ.T)
\end{lstlisting}

 And the matrix obtained from the above process would look something like this:
\begin{table}[ht]
\caption{Output of Matrix Factorization}
\centering % used for centering table
\begin{tabular}{c c c c c}
\hline\hline %inserts double horizontal lines
& D1 & D2 & D3 & D4 \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
U1 & 4.97 & 2.98 & 2.18 & 0.98\\ % inserting body of the table
U2 & 3.97 & 2.40 & 1.97 & 0.99\\
U3 & 1.02 & 0.93 & 5.32 & 4.93\\
U4 & 1.00 & 0.85 & 4.59 & 3.93\\
U5 & 1.36 & 1.07 & 4.89 & 4.12\\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

 We can see that for existing ratings we have the approximations very close to the true values, and we also get some 'predictions' of the unknown values. In this simple example, we can easily see that U1 and U2 have similar taste and they both rated D1 and D2 high, while the rest of the users preferred D3, D4 and D5. When the number of features (K in the Python code) is 2, the algorithm is able to associate the users and items to two different features, and the predictions also follow these associations. For example, we can see that the predicted rating of U4 on D3 is 4.59, because U4 and U5 both rated D4 high.

