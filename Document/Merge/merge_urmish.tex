\section{Model Based Systems}
  The systems based on methods discussed earlier in the lab have been widely used and produce good results. But such algorithms have been shown to have several limitations. 
  \begin{itemize}
    \item \underline{Sparsity} - Nearest Neighbor (NN) Algorithms need exact matches. As a result algorithms sacrifice recommender system coverage and accuracy [TODO FIXME ref1]. To be more precise, correlation coefficient is only defined between customers who have products (movies in our database) in common. In an ecommerce environment where there are large number of items, one may find many customers who do not have any correlation with other customers [TODO FIXME ref2]. As a result Nearest Neighbor based algorithms are not able to recommend anything to these customers. This problem is known as the reduced coverage problem. The sparsity could also lead to a recommender system from missing certain obvious neighbors. 

\textit{For example - John and Susan are highly correlated, while Susan and Jack are highly correlated. Conventional wisdom might suggest John and Jack should also have similar choices, however if John and Jack have very few ratings in common, such patterns could be easily missed}
    \item \underline{Synonym Problem} - In real life scenarios, different product names can refer to similar items. Correlation based recommender systems cannot find such latent association and thus end up treating these objects as two seperate entities. 

\textit{For example, let us consider two customers one of whom rates 10 different writing pad products as "high" and another customer rates 10 different notepads as "high". NN based recommender system will not capture their association.}
  \end{itemize}
  One of the methods used to handle both the problems is the low rank approximation method. Let us do an exercise to understand the issues discussed above and see how low rank approximation can help us. \\
  \begin{table}[]
  \centering
  \label{my-label}
  \begin{tabular}{|l|l|l|l|l|}
  \hline
      & Die Hard 1 & Die Hard 2 & Die Hard 3 & Die Hard 4 \\ \hline
John  & 4          & 4          &            &            \\ \hline
Susan & 4          & 2          & 3          & 3          \\ \hline
Jack  &            &            & 4          & 4          \\ \hline
  \end{tabular}
  \caption{Truncate to Die Hard}
  \label{Model-1}
  \end{table}
\\
\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Take a look at Table~\ref{Model-1}. The table illustrates the example we described while discussing \textit{Sparsity}. So how do you ensure that John and Eric are considered similar to each other? Look at the code written in \textit{Examples/ToyExampleModelSVDMotivation.m} and run it to see how low rank approximation helps us magically establish this relationship between John and Eric!! We will discuss what low rank approximation in the next section.  For now, do you agree that this method is helping us with the sparsity problem? 
\end{myremark}

\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Look at \textit{Examples/ToyExampleModelSVDMotivation2.m}. It explains a scenario similar to the talked about in Synonym Problem and looks at a potential solution. Run the code and observe the output? Do you think Low Rank approximation is finding some sort of latent association? What are the predictions for values not rated by the user?
\end{myremark}
  \subsection{Singular Vector Decomposition}

The Singular Value Decomposition (SVD) is a well known matrix factorization method. Formally, the SVD of a $m \times n$ matrix $A$ is a factorization of the form $A = U \Sigma V^{T}$ where $U$ is a $m \times m$ orthogonal matrix, $\Sigma$ is a $m \times n$ diagonal matrix with non-negative terms on the diagonal, and $V$ is a $n \times n$ orthogonal matrix. The diagonal entries of $\Sigma$ are known as the singular values of $A$. The $m$ columns of $U$ and the $n$ column of $V$ are known as the left and right singular vectors of $A$ respectively. 

The SVD gives the `best' low-rank approximation of a matrix. To put this in a more formal notation, it minimizes the Frobenius form of the difference between the approximation and the original matrix.

Assume that matrix $A \in R^{m \times n}$ with rank $r > k$. The Frobenius norm approximation problem $min || A - Z ||_{F}$ where $rank(Z) = k$ has the solution:
\[Z = A_{k} = U_{k} \Sigma_{k} V_{k}^{T}\]
where $U_{k}, \Sigma_{k}$ and $V_{k}$ are the matrices obtained by truncating the SVD to contain only the first k singular vectors/values.

The SVD is implemented by the function $svd$ in MATLAB. You can try the following code in matlab to get an idea:
\begin{verbatim}
A = magic(3)
[U,D,V] = svd(A)
\end{verbatim}

You should observe that the columns of the matrices $U$ and $V$ are orthonormal. Also note that $\Sigma$ is a diagonal matrix with non-negative and monotonically decreasing entries along the diagonal. These are the singular values of $A$.
  \subsection{Generating Predictions}
  We can use SVD in recommender system to capture certain latent relationship between customers and products/movies. We can use this relationship to capture the predicted likeliness of a certain product/movie by a consumer. \textit{So the next big question is}, how do you calculate the SVD of an incomplete matrix? Or rather, how do you complete a matrix in general? In order to handle this, we will study various approximations. We will then run these methods and collect empirical evidence to validate their effectiveness.
  \subsubsection{Latent Matrix Factorization}
 There are numerous algorithms for generating recommendations. While user-based or item-based collaborative filtering methods are easy and intuitive, matrix factorization methods are usually more efficient because they allow us to determine the latent features underlying the interactions between users and items. Matrix factorization is just a mathematical tool for playing around with matrices, and is thus relevant in many situations where one would like to discover something hidden in the data.
 Matrix factorization is basically factorizing a matrix, i.e. finding out two (or more) matrices such that their multiplication will result in the original matrix.
 It can be used to identify latent features underlying the interactions between two different kinds of entities. One of it’s obvious application is to predict ratings (or score) in collaborative filtering.
  \subsubsection{Average Value Based SVD Completion}
  Below is the description of steps used to generate predictions using Average Values -
  \begin{enumerate}
    \item Let $R$ be the original sparse matrix where rows represent the user and columns represent the movies
    \item Fill the empty cells in each column with average values of the product/movies in that column.
    \item Calculate the average rating for each customer $\overline{C}$ using the non zero values
    \item Let $R_{norm} = R - \overline{C}$, i.e. mean center each row. 
    \item Factor $R_{norm}$ using SVD and obtain $U$, $S$ and $V$
    \item Truncate $U$, $S$ and $V$ to $U_k$, $S_k$ and $V_k$.
    \item Compute the resultant matrix $Pred_{norm} = U_k*S_k*V_k$
    \item Add the average customer value calculated in Step 3 to this matrix, i.e, $Pred = Pred_{norm} + \overline{C} $
  \end{enumerate}
\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Implement a function for average value SVD completion. Use the steps described above and run the code on matrix generated from table \ref{table:example_ratings1}. Compare the result with our implementation located in \textit{Examples/ToyExampleAverageValueSVD.m}. What do you think about the corresponding predictions? Do they make sense? 
\end{myremark}
  \subsubsection{Iterative SVD Completion}
  Another simple approach to complete a matrix and generate predictions is to use iterative SVD. 
  \begin{enumerate}
    \item Let $R$ be the original sparse matrix where rows represent the user and columns represent the movies
    \item Fill the empty cells in each row with 0. Let this new Matrix be $R_{next}$
    \item Factorize $R_{next}$ using SVD and obtain $U$, $S$ and $V$
    \item Truncate $U$, $S$ and $V$ to $U_k$, $S_k$ and $V_k$.
    \item Compute the resultant matrix $R_{next} = U_k*S_k*V_k$
    \item Use the known values in $R$ to replace values in $R_{next}$
    \item Repeat the process from Step 3, $T$ number of times. $T$ is the number of times you want to repeat the process.
  \end{enumerate}
\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Implement a function for iterative SVD completion. Use the steps described above and run your code on the matrix generated from table \ref{table:example_ratings1}. Compare the result with our implementation located in \textit{Examples/ToyExampleIterativeSVD.m}. Look at the corresponding predictions. What do you think about them? How do they compare to the predictions using the previous methods?
\end{myremark}
  \subsection{Nearest Neighbor}
  By now you should have learnt to predict the ratings for unmarked items. So what do we do with these predictions? One thing we can do is recommend the top rated predictions for each user and that works perfectly fine. Another thing we can look into is the Nearest Neighbor approach that we discussed earlier in the lab.
  \subsubsection{Using Predicted Matrix}
  Once we have a prediction for all the movies/products of all the users, we can use nearest neighbors to generate item recommendation for a user. Why do we do this when we already have a prediction? Well, nearest neighbor method can lead to something called as "Serendipity". We will discuss the meaning of Serendipity in a later section. This method is based on the assumption - given a high accuracy of prediction, we might find better neighbors and thus better ratings.
  \subsubsection{Original Matrix in Lower Dimension}
  Another work around to find better neighbors is to look for neighbors in a lower dimension space of the original sparse matrix. The motivation here is that a lower dimension space will lead to a denser matrix and might help us find better neighbors.
  \subsubsection{Which Nearest Neighbor approximation to Use?}
  To answer this question we will resort to the golden answer to anything non-concrete - \textit{"It Depends!!!"}. For the purpose of this lab we used Nearest Neighbor approach in a lower dimension space of predicted matrix. Empirically, it worked better. In general, one tries different approaches and sees what works best for their environment and their preference of error metrics.
  \subsection{Bringing it All Together}
  Now that you have all the concepts required to understand a Model Based Recommender System, we will encourage you to experiment with it to get an idea of the different trade-offs involved and the corresponding complexity in settling down on a method.
\begin{myremark}{\stepcounter{ques}Exercise \arabic{ques}}
Write a code to run Model Based Prediction on the entire movie lens dataset. Follow these steps - 
\begin{enumerate}
\item Load u1.base and u1.test.
\item Create a Matrix of the form user*movies where unitialized values to zero. We already have function calls to do both these steps so feel free to use them
\item Create two matrix completion for the u1.base dataset, one based on Average Value and one based on Incremental SVD
\item Look at the Top 5 predictions from both of them for any random customer.
\item Pick a customer from u1.test and look at the movies he has rated. 
\item Look at the corresponding ratings in your predictions. Do they agree?
\item Calculate the various error metrics that we discussed earlier in the lab.
\end{enumerate}
We have already implemented this code in \textit{ModelBasedPrediction / EvaluateModelBasedSVD.m}. Play around with the parameters, look at the impact it has on different error rates. It is upto you to decide what factors are important and what constitutes a good system. We have provided you with outputs for many different parameters in the table-reference-here[TODO FIXME].
\end{myremark}
  \section{Miscellaneous Topics}
  \subsection{Content Based Recommender Systems}

 Content-based recommender systems recommend an item to a user based upon a description of the item and the profile of the user’s interests. Systems implementing a content-based recommendation approach analyze a set of documents and/or descriptions of items previously rated by a user, and build a model or profile of user interests based on the features of the objects rated by that user. The profile is a structured representation of user interests, adopted to recommend new interesting items. The recommendation process basically consists in matching up the attributes of the user profile against the attributes of a content object. The result is a relevance judgment that represents the user’s level of interest in that object. If a profile accurately reflects user preferences, it is of tremendous advantage for the effectiveness of an information access process. For instance, it could be used to filter search results by deciding whether a user is interested in a specific Web page or not and, in the negative case, preventing it from being displayed.
\subsubsection{Advantages and Drawbacks of Content-based Filtering}

 The adoption of the content-based recommendation paradigm has several advantages when compared to the collaborative one:
\begin{itemize}
\item \textbf{User Independence} - Content-based recommenders exploit solely ratings provided by the active user to build her own profile.
\end{itemize}
\begin{itemize}
\item \textbf{Transparency} - Explanations on how the recommender system works can be provided by explicitly listing content features or descriptions that caused an item to occur in the list of recommendations. 
\end{itemize}
\begin{itemize}
\item \textbf{New Item} - Content-based recommenders are capable of recommending items not yet rated by any user i.e., they do not suffer from the first-rater problem.
\end{itemize}

 Nonetheless, content-based systems have several shortcomings:
\begin{itemize}
\item \textbf{Limited Content Analysis} - Content-based techniques have a natural limit in the number and type of features that are associated with the objects they recommend. Domain knowledge is often needed, e.g., for movie recommendations the system needs to know the actors and directors, and sometimes, domain ontologies are also needed. No content-based recommendation system can provide suitable suggestions if the analyzed content does not contain enough information to discriminate items the user likes from items the user does not like.
\end{itemize}
\begin{itemize}
\item \textbf{Over-Specialization} - Content-based recommenders have no inherent method for finding something unexpected. The system suggests items whose scores are high when matched against the user profile, hence the user is going to be recommended items similar to those already rated. This drawback is also called serendipity problem to highlight the tendency of the content-based systems to produce recommendations with a limited degree of novelty.
\end{itemize}
\begin{itemize}
\item \textbf{New User} - Enough ratings have to be collected before a content-based recommender system can really understand user preferences and provide accurate recommendations. Therefore, when few ratings are available, as for a new user, the system will not be able to provide reliable recommendations.
\end{itemize}

  \subsection{Serendipity}
  Recommender Systems exist to help users discover an item that he or she would like among the large pool of items available. Most of the errors that we saw did not talk about the novelty of recommendation. For example, if I have seen Star Wars: A New Hope, Star Wars: The Empire Strikes Back and Star Wars: Revenge of the Sith, I will most likely find a lot of nearest neighbors whose collective votes leads to the recommendation for The Phantom Menace, Attack of the Clones and Revenge of the Sith. I will definitely end up liking that movie. Now consider a scenario where watch a movie of a completely different genre, say \textit{Lincoln} and enjoy it a lot. An item based recommendation system would have never led me to discover this movie as all the movies I have rated fall into the Action/Saga/Fantasy category. Such discoveries are called serendipitous discoveries. 

Serendipity is related to unexpectedness and involves a positive emotional response of the user about a previously unknown item. It is concerned with the novelty of recommendations and how far such recommendations may positively affect the user. Serendipity is generally not easy to measure and most systems consider a trade-off between various forms of accuracy and serendipity. 
% \end{document}

